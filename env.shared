LLM_PROVIDER="ollama"

SKIP_LANG_TRANSLATOR=False

DEFAULT_TRANSLATION_LANG="French"
DEFAULT_EXPORT_TRANSCRIBER="distil_whisper"

## openai-server
OPENAI_API_HOST="http://192.168.0.29:8004"
## ollama
OLLAMA_API_HOST="http://192.168.0.18:12345"

## data storage
HISTORY_DB="resources/historydb"
SESSION_LOGS="workspace/session_logs"
DOWNLOADS_DIR="workspace/downloads"

## huggingface models
HF_CACHE_DIR="resources/models"

## Utility Models
LANG_TRANSLATOR="facebook/seamless-m4t-v2-large"
DISTIL_WHISPER="distil-whisper/distil-large-v2"
FASTER_WHISPER="Systran/faster-whisper-large-v3"

LLAMACPP_MODEL="multimodal"

## LLM Local - Multimodal 
LOCAL_MM_REPO_ID="cjpais/llava-1.6-mistral-7b-gguf"
LOCAL_MM_REPO_MODEL_FILE="llava-v1.6-mistral-7b.Q4_K_M.gguf"
LOCAL_MM_REPO_PROJECT_FILE="mmproj-model-f16.gguf"
LOCAL_MM_REPO_CHAT_FORMAT="llava"

## LLM Local - mistral 7b
LOCAL_DEFAULT_REPO_ID="TheBloke/Mistral-7B-Instruct-v0.2-GGUF"
LOCAL_DEFAULT_REPO_MODEL_FILE="mistral-7b-instruct-v0.2.Q4_K_M.gguf"
LOCAL_DEFAULT_REPO_CHAT_FORMAT="llama-2"

## LLM Local - functionary
LOCAL_FUN_REPO="meetkai/functionary-7b-v2"
LOCAL_FUN_REPO_ID="meetkai/functionary-small-v2.2-GGUF"
LOCAL_FUN_REPO_MODEL_FILE="functionary-small-v2.2.q4_0.gguf"
LOCAL_FUN_REPO_CHAT_FORMAT="functionary-v2"

